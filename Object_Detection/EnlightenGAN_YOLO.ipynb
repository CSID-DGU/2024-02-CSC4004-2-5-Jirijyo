{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **yaml 작성**\n",
        "\n",
        "*   data_origin.yaml\n",
        "*   data_enlightengan_moderate.yaml\n",
        "*   data_enlightengan_severe.yaml\n",
        "*   data_enlightengan_extreme.yaml\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jvEvHmAHs3wl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e533eb4d-1731-49c0-b7ed-ed9bf3588c3a",
        "id": "671BpSVXjJKA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train: /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Severe/enlightengan/images\n",
            "val: /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Severe/enlightengan/images\n",
            "test: ..\n",
            "\n",
            "nc: 1\n",
            "names: ['Human pose estimation']\n",
            "\n",
            "roboflow:\n",
            "  workspace: project-wk4fq\n",
            "  project: cctv-people\n",
            "  version: 1\n",
            "  license: CC BY 4.0\n",
            "  url: https://universe.roboflow.com/project-wk4fq/cctv-people/dataset/1\n"
          ]
        }
      ],
      "source": [
        "# data_enlightengan_severe.yaml 파일 작성\n",
        "yaml_content = \"\"\"\n",
        "train: /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Severe/enlightengan/images\n",
        "val: /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Severe/enlightengan/images\n",
        "test: ..\n",
        "\n",
        "nc: 1\n",
        "names: ['Human pose estimation']\n",
        "\n",
        "roboflow:\n",
        "  workspace: project-wk4fq\n",
        "  project: cctv-people\n",
        "  version: 1\n",
        "  license: CC BY 4.0\n",
        "  url: https://universe.roboflow.com/project-wk4fq/cctv-people/dataset/1\n",
        "\"\"\"\n",
        "\n",
        "# 파일 저장\n",
        "with open('/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_severe.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "# 저장된 파일 확인\n",
        "!cat /content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_severe.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84812f2d-f689-4cfc-92c3-2be60697aa25",
        "id": "pEFFIgufzbOH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train: /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Extreme/enlightengan/images\n",
            "val: /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Extreme/enlightengan/images\n",
            "test: ..\n",
            "\n",
            "nc: 1\n",
            "names: ['Human pose estimation']\n",
            "\n",
            "roboflow:\n",
            "  workspace: project-wk4fq\n",
            "  project: cctv-people\n",
            "  version: 1\n",
            "  license: CC BY 4.0\n",
            "  url: https://universe.roboflow.com/project-wk4fq/cctv-people/dataset/1\n"
          ]
        }
      ],
      "source": [
        "# data_enlightengan_extreme.yaml 파일 작성\n",
        "yaml_content = \"\"\"\n",
        "train: /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Extreme/enlightengan/images\n",
        "val: /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Extreme/enlightengan/images\n",
        "test: ..\n",
        "\n",
        "nc: 1\n",
        "names: ['Human pose estimation']\n",
        "\n",
        "roboflow:\n",
        "  workspace: project-wk4fq\n",
        "  project: cctv-people\n",
        "  version: 1\n",
        "  license: CC BY 4.0\n",
        "  url: https://universe.roboflow.com/project-wk4fq/cctv-people/dataset/1\n",
        "\"\"\"\n",
        "\n",
        "# 파일 저장\n",
        "with open('/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_extreme.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "# 저장된 파일 확인\n",
        "!cat /content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_extreme.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **yolo**"
      ],
      "metadata": {
        "id": "7eXyR016vE7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train**"
      ],
      "metadata": {
        "id": "IlHaMTMWjeJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRVAj-8AqGdu",
        "outputId": "45ee1204-a03a-4193-e48d-54736e213a66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4peng09RqOvL",
        "outputId": "eebfd10e-f795-4b9f-c3d9-500fdc8b00cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 32.6/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 원본 라벨 폴더\n",
        "labels_folder = \"/content/drive/MyDrive/CCTV-People-Dataset/train/labels\"\n",
        "\n",
        "# 저조도 이미지 폴더들\n",
        "low_brightness_folders = [\n",
        "    \"/content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Moderate/enlightengan\",\n",
        "    \"/content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Severe/enlightengan\",\n",
        "    \"/content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Extreme/enlightengan\",\n",
        "]\n",
        "\n",
        "# 각 저조도 폴더에 라벨 복사\n",
        "for folder in low_brightness_folders:\n",
        "    images_folder = os.path.join(folder, \"images\")\n",
        "    labels_output_folder = os.path.join(folder, \"labels\")\n",
        "\n",
        "    # 라벨 저장 폴더가 없으면 생성\n",
        "    os.makedirs(labels_output_folder, exist_ok=True)\n",
        "\n",
        "    # 이미지 파일 이름에 맞는 라벨 복사\n",
        "    for image_file in os.listdir(images_folder):\n",
        "        if image_file.endswith(\".jpg\"):  # 이미지 파일만 처리\n",
        "            label_file = image_file.replace(\".jpg\", \".txt\")  # 라벨 파일 이름\n",
        "            src_label_path = os.path.join(labels_folder, label_file)\n",
        "            dst_label_path = os.path.join(labels_output_folder, label_file)\n",
        "\n",
        "            # 라벨 파일 복사\n",
        "            if os.path.exists(src_label_path):\n",
        "                shutil.copy(src_label_path, dst_label_path)\n",
        "            else:\n",
        "                print(f\"Label not found for {image_file}\")"
      ],
      "metadata": {
        "id": "isZHdfvHz7dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 원본 라벨 폴더\n",
        "labels_folder = \"/content/drive/MyDrive/CCTV-People-Dataset/valid/labels\"\n",
        "\n",
        "# 저조도 이미지 폴더들\n",
        "low_brightness_folders = [\n",
        "    \"/content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Moderate/enlightengan\",\n",
        "    \"/content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Severe/enlightengan\",\n",
        "    \"/content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Extreme/enlightengan\",\n",
        "]\n",
        "\n",
        "# 각 저조도 폴더에 라벨 복사\n",
        "for folder in low_brightness_folders:\n",
        "    images_folder = os.path.join(folder, \"images\")\n",
        "    labels_output_folder = os.path.join(folder, \"labels\")\n",
        "\n",
        "    # 라벨 저장 폴더가 없으면 생성\n",
        "    os.makedirs(labels_output_folder, exist_ok=True)\n",
        "\n",
        "    # 이미지 파일 이름에 맞는 라벨 복사\n",
        "    for image_file in os.listdir(images_folder):\n",
        "        if image_file.endswith(\".jpg\"):  # 이미지 파일만 처리\n",
        "            label_file = image_file.replace(\".jpg\", \".txt\")  # 라벨 파일 이름\n",
        "            src_label_path = os.path.join(labels_folder, label_file)\n",
        "            dst_label_path = os.path.join(labels_output_folder, label_file)\n",
        "\n",
        "            # 라벨 파일 복사\n",
        "            if os.path.exists(src_label_path):\n",
        "                shutil.copy(src_label_path, dst_label_path)\n",
        "            else:\n",
        "                print(f\"Label not found for {image_file}\")"
      ],
      "metadata": {
        "id": "mP4t7cRp0V0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo train model=yolo11n.pt \\\n",
        "          data=/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_moderate.yaml \\\n",
        "          epochs=25 \\\n",
        "          imgsz=640 \\\n",
        "          batch=16 \\\n",
        "          save_period=5 \\\n",
        "          lr0=0.001 \\\n",
        "          optimizer=auto \\\n",
        "          project=/content/drive/MyDrive/CCTV-People-Dataset/runs/train \\\n",
        "          name=enlightengan_moderate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwPOxjgVx-Vn",
        "outputId": "779ea84b-fe78-4949-d65c-3d09359ef57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 85.2MB/s]\n",
            "Ultralytics 8.3.40 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_moderate.yaml, epochs=25, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=/content/drive/MyDrive/CCTV-People-Dataset/runs/train, name=enlightengan_moderate, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 17.6MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Moderate/enlightengan/labels... 525 images, 0 backgrounds, 0 corrupt: 100% 525/525 [00:06<00:00, 82.95it/s] \n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Moderate/enlightengan/labels.cache\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Moderate/enlightengan/labels... 175 images, 0 backgrounds, 0 corrupt: 100% 175/175 [00:03<00:00, 51.44it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Moderate/enlightengan/labels.cache\n",
            "Plotting labels to /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/25      2.48G      2.144      3.313      1.773         26        640: 100% 33/33 [00:17<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.58it/s]\n",
            "                   all        175        227    0.00358      0.828      0.161     0.0572\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/25      2.36G      2.055      2.812      1.733         21        640: 100% 33/33 [00:13<00:00,  2.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.99it/s]\n",
            "                   all        175        227       0.17     0.0573     0.0455     0.0117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/25      2.36G      2.077      2.687      1.754         30        640: 100% 33/33 [00:15<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.55it/s]\n",
            "                   all        175        227      0.783     0.0161     0.0698     0.0223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/25      2.38G      2.096      2.489      1.814         29        640: 100% 33/33 [00:14<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.28it/s]\n",
            "                   all        175        227      0.595      0.339      0.391      0.147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/25      2.36G      2.027      2.342      1.767         20        640: 100% 33/33 [00:14<00:00,  2.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.28it/s]\n",
            "                   all        175        227      0.527      0.357      0.377      0.127\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/25      2.38G      1.947      2.161       1.73         27        640: 100% 33/33 [00:13<00:00,  2.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.36it/s]\n",
            "                   all        175        227      0.347      0.399      0.327      0.124\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/25      2.36G      1.932      2.091      1.693         32        640: 100% 33/33 [00:14<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.48it/s]\n",
            "                   all        175        227      0.257      0.185       0.16     0.0533\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/25      2.36G      1.923      2.046      1.716         22        640: 100% 33/33 [00:18<00:00,  1.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.07it/s]\n",
            "                   all        175        227      0.596      0.304        0.3      0.125\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/25      2.36G      1.931      1.918       1.71         27        640: 100% 33/33 [00:14<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.22it/s]\n",
            "                   all        175        227      0.604      0.599      0.597      0.261\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/25      2.37G      1.926      1.867      1.722         29        640: 100% 33/33 [00:14<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.26it/s]\n",
            "                   all        175        227      0.627      0.495      0.534      0.265\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/25      2.37G      1.822      1.797      1.663         30        640: 100% 33/33 [00:14<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.62it/s]\n",
            "                   all        175        227      0.696      0.565      0.645      0.306\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/25      2.37G      1.796      1.737      1.645         24        640: 100% 33/33 [00:14<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.69it/s]\n",
            "                   all        175        227      0.728      0.576      0.616      0.297\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/25      2.36G      1.811      1.809       1.64         21        640: 100% 33/33 [00:14<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.58it/s]\n",
            "                   all        175        227      0.632      0.598      0.629      0.302\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/25      2.37G      1.757      1.664      1.589         27        640: 100% 33/33 [00:15<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.93it/s]\n",
            "                   all        175        227      0.837      0.567      0.687      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/25      2.36G       1.71      1.579       1.54         20        640: 100% 33/33 [00:14<00:00,  2.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.77it/s]\n",
            "                   all        175        227      0.777      0.546      0.642       0.33\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/25      2.43G      1.698      1.624      1.644         17        640: 100% 33/33 [00:16<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.98it/s]\n",
            "                   all        175        227      0.832      0.436      0.524      0.262\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/25      2.36G      1.622      1.443      1.562         18        640: 100% 33/33 [00:14<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.50it/s]\n",
            "                   all        175        227      0.638      0.555       0.61      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/25      2.37G      1.556      1.418       1.57         17        640: 100% 33/33 [00:14<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.63it/s]\n",
            "                   all        175        227      0.815       0.62      0.717      0.369\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/25      2.36G      1.555      1.344      1.555         19        640: 100% 33/33 [00:17<00:00,  1.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.26it/s]\n",
            "                   all        175        227      0.832      0.683      0.778      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/25      2.36G      1.559      1.279       1.55         19        640: 100% 33/33 [00:14<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.81it/s]\n",
            "                   all        175        227      0.818      0.674      0.771       0.41\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/25      2.36G      1.499      1.219      1.488         16        640: 100% 33/33 [00:14<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.85it/s]\n",
            "                   all        175        227      0.774      0.726      0.779      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/25      2.37G      1.442      1.208      1.459         16        640: 100% 33/33 [00:14<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.52it/s]\n",
            "                   all        175        227      0.848      0.727       0.79      0.456\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/25      2.36G       1.43       1.16      1.454         15        640: 100% 33/33 [00:14<00:00,  2.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.60it/s]\n",
            "                   all        175        227      0.836      0.727      0.801      0.454\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/25      2.36G      1.403      1.141      1.414         16        640: 100% 33/33 [00:13<00:00,  2.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.30it/s]\n",
            "                   all        175        227      0.881       0.72      0.803      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/25      2.36G      1.359      1.092      1.395         15        640: 100% 33/33 [00:14<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.94it/s]\n",
            "                   all        175        227      0.893      0.749      0.825       0.47\n",
            "\n",
            "25 epochs completed in 0.135 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate/weights/best.pt...\n",
            "Ultralytics 8.3.40 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.35it/s]\n",
            "                   all        175        227      0.892      0.749      0.825       0.47\n",
            "Speed: 0.3ms preprocess, 3.8ms inference, 0.0ms loss, 6.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo train model=yolo11n.pt \\\n",
        "          data=/content/drives/MyDrive/CCTV-People-Dataset/yaml/data_severe_GAN.yaml \\\n",
        "          epochs=25 \\\n",
        "          imgsz=640 \\\n",
        "          batch=16 \\\n",
        "          save_period=5 \\\n",
        "          lr0=0.001 \\\n",
        "          optimizer=auto \\\n",
        "          project=/content/drives/MyDrive/CCTV-People-Dataset/runs/train \\\n",
        "          name=severe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D8AixW00zrG",
        "outputId": "20dd8dbd-3a49-4895-fec6-85b9d60f418a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/content/drives/MyDrive/CCTV-People-Dataset/yaml/data_severe_GAN.yaml, epochs=25, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=/content/drives/MyDrive/CCTV-People-Dataset/runs/train, name=severe2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "WARNING ⚠️ NMS time limit 2.400s exceeded\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drives/.shortcut-targets-by-id/1jzPGHpihxLlAVVUeo7WsMtf8Oexq4qSH/CCTV-People-Dataset/train/Low_Brightness_Severe/GAN_S/labels.cache... 525 images, 0 backgrounds, 0 corrupt: 100% 525/525 [00:00<?, ?it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drives/.shortcut-targets-by-id/1jzPGHpihxLlAVVUeo7WsMtf8Oexq4qSH/CCTV-People-Dataset/valid/Low_Brightness_Severe/GAN_S_v/labels.cache... 175 images, 0 backgrounds, 0 corrupt: 100% 175/175 [00:00<?, ?it/s]\n",
            "Plotting labels to /content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/25      2.56G      2.221      3.399      1.823         26        640: 100% 33/33 [00:33<00:00,  1.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.55it/s]\n",
            "                   all        175        227    0.00401      0.559      0.159     0.0486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/25      2.36G      2.065      2.789      1.751         21        640: 100% 33/33 [00:26<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.48it/s]\n",
            "                   all        175        227      0.622     0.0176     0.0797     0.0269\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/25      2.36G      2.097      2.708      1.782         30        640: 100% 33/33 [00:23<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.03s/it]\n",
            "                   all        175        227      0.163      0.167     0.0652     0.0184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/25      2.38G      2.074      2.555      1.765         29        640: 100% 33/33 [00:22<00:00,  1.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.02it/s]\n",
            "                   all        175        227      0.391      0.225       0.23     0.0979\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/25      2.36G      2.021      2.502      1.749         20        640: 100% 33/33 [00:23<00:00,  1.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.36it/s]\n",
            "                   all        175        227      0.147      0.229     0.0884     0.0334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/25      2.38G       2.04       2.28      1.755         27        640: 100% 33/33 [00:24<00:00,  1.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.36it/s]\n",
            "                   all        175        227      0.298      0.242      0.204     0.0874\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/25      2.36G      2.028      2.189      1.756         32        640: 100% 33/33 [00:26<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.36it/s]\n",
            "                   all        175        227      0.512      0.392      0.419      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/25      2.37G       1.96      2.122      1.734         22        640: 100% 33/33 [00:23<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:07<00:00,  1.18s/it]\n",
            "                   all        175        227      0.655       0.41      0.486      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/25      2.36G      1.957      1.992      1.726         27        640: 100% 33/33 [00:23<00:00,  1.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.04it/s]\n",
            "                   all        175        227      0.444      0.467      0.427       0.19\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/25      2.37G      1.915      1.941      1.702         29        640: 100% 33/33 [00:22<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.44it/s]\n",
            "                   all        175        227      0.574      0.498      0.513      0.257\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/25      2.37G      1.877      1.859      1.674         30        640: 100% 33/33 [00:24<00:00,  1.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.40it/s]\n",
            "                   all        175        227      0.675      0.503      0.551      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/25      2.37G      1.829       1.74      1.616         24        640: 100% 33/33 [00:26<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.61it/s]\n",
            "                   all        175        227       0.76      0.476      0.561       0.26\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/25      2.36G      1.876      1.838      1.656         21        640: 100% 33/33 [00:23<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.06s/it]\n",
            "                   all        175        227      0.654      0.482      0.526      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/25      2.37G      1.762      1.702      1.595         27        640: 100% 33/33 [00:23<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.09it/s]\n",
            "                   all        175        227       0.78      0.515      0.643       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/25      2.36G      1.795      1.641      1.597         20        640: 100% 33/33 [00:22<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.31it/s]\n",
            "                   all        175        227      0.779      0.542      0.651       0.32\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/25      2.54G      1.716      1.699      1.616         17        640: 100% 33/33 [00:27<00:00,  1.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.30it/s]\n",
            "                   all        175        227       0.62      0.532      0.543      0.272\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/25      2.36G      1.645      1.514      1.556         18        640: 100% 33/33 [00:24<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.54it/s]\n",
            "                   all        175        227      0.746      0.546       0.63      0.335\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/25      2.37G      1.629      1.442      1.595         17        640: 100% 33/33 [00:27<00:00,  1.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.31it/s]\n",
            "                   all        175        227      0.787      0.537      0.658      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/25      2.36G      1.594      1.388      1.561         19        640: 100% 33/33 [00:24<00:00,  1.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.07s/it]\n",
            "                   all        175        227      0.728      0.599      0.688      0.357\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/25      2.36G      1.608      1.355      1.567         19        640: 100% 33/33 [00:23<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.12it/s]\n",
            "                   all        175        227      0.857       0.58      0.729      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/25      2.36G      1.538      1.301      1.499         16        640: 100% 33/33 [00:24<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.54it/s]\n",
            "                   all        175        227      0.782      0.656      0.737      0.402\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/25      2.37G      1.468      1.266      1.444         16        640: 100% 33/33 [00:27<00:00,  1.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.38it/s]\n",
            "                   all        175        227      0.811      0.683      0.761      0.417\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/25      2.36G      1.484      1.214      1.459         15        640: 100% 33/33 [00:22<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.08s/it]\n",
            "                   all        175        227      0.868      0.668      0.769      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/25      2.36G      1.463      1.184      1.433         16        640: 100% 33/33 [00:23<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.36it/s]\n",
            "                   all        175        227      0.861      0.683       0.78      0.434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/25      2.36G      1.406      1.154      1.404         15        640: 100% 33/33 [00:24<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.75it/s]\n",
            "                   all        175        227      0.864      0.661      0.785       0.44\n",
            "\n",
            "25 epochs completed in 0.237 hours.\n",
            "Optimizer stripped from /content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from /content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating /content/drives/MyDrive/CCTV-People-Dataset/runs/train/severe2/weights/best.pt...\n",
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33% 2/6 [00:02<00:05,  1.47s/it]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo train model=yolo11n.pt \\\n",
        "          data=/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_extreme.yaml \\\n",
        "          epochs=25 \\\n",
        "          imgsz=640 \\\n",
        "          batch=16 \\\n",
        "          save_period=5 \\\n",
        "          lr0=0.001 \\\n",
        "          optimizer=auto \\\n",
        "          project=/content/drive/MyDrive/CCTV-People-Dataset/runs/train \\\n",
        "          name=enlightengan_extreme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce398c0-31a6-4b22-f923-843606b7411a",
        "id": "id-Uo0RtWLb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0.00/5.35M [00:00<?, ?B/s]\r100% 5.35M/5.35M [00:00<00:00, 84.9MB/s]\n",
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_extreme.yaml, epochs=25, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=/content/drive/MyDrive/CCTV-People-Dataset/runs/train, name=enlightengan_extreme, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme\n",
            "100% 755k/755k [00:00<00:00, 17.2MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "WARNING ⚠️ NMS time limit 2.400s exceeded\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Extreme/enlightengan/labels... 525 images, 0 backgrounds, 0 corrupt: 100% 525/525 [00:27<00:00, 19.43it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/CCTV-People-Dataset/train/Low_Brightness_Extreme/enlightengan/labels.cache\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Extreme/enlightengan/labels... 175 images, 0 backgrounds, 0 corrupt: 100% 175/175 [00:08<00:00, 19.74it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Extreme/enlightengan/labels.cache\n",
            "Plotting labels to /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/25      2.48G      2.419      3.707      1.987         26        640: 100% 33/33 [00:28<00:00,  1.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.01s/it]\n",
            "                   all        175        227    0.00301      0.696     0.0641     0.0153\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/25      2.36G      2.261      3.173      1.905         21        640: 100% 33/33 [00:22<00:00,  1.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.01s/it]\n",
            "                   all        175        227     0.0893     0.0393     0.0192    0.00746\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/25      2.36G       2.27      3.034      1.955         30        640: 100% 33/33 [00:20<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.63it/s]\n",
            "                   all        175        227      0.257     0.0837     0.0634     0.0188\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/25      2.38G      2.183      2.714      1.886         29        640: 100% 33/33 [00:21<00:00,  1.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.08it/s]\n",
            "                   all        175        227      0.368      0.207      0.189     0.0769\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/25      2.36G      2.177      2.598      1.873         20        640: 100% 33/33 [00:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.58it/s]\n",
            "                   all        175        227      0.321        0.3      0.217     0.0785\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/25      2.39G      2.125      2.451      1.893         27        640: 100% 33/33 [00:22<00:00,  1.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.40it/s]\n",
            "                   all        175        227      0.384      0.304      0.283      0.099\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/25      2.37G      2.094      2.398      1.841         32        640: 100% 33/33 [00:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.61it/s]\n",
            "                   all        175        227      0.321      0.291       0.21     0.0775\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/25      2.37G      2.027      2.284      1.825         22        640: 100% 33/33 [00:22<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.37it/s]\n",
            "                   all        175        227      0.606      0.339       0.41      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/25      2.36G      2.012      2.205      1.791         27        640: 100% 33/33 [00:24<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.03it/s]\n",
            "                   all        175        227      0.533      0.405      0.418      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/25      2.37G      2.013      2.177      1.794         29        640: 100% 33/33 [00:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.67it/s]\n",
            "                   all        175        227      0.505      0.427      0.411      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/25      2.37G      1.952      2.063      1.742         30        640: 100% 33/33 [00:21<00:00,  1.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.05it/s]\n",
            "                   all        175        227       0.66      0.405      0.468      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/25      2.37G      1.891       1.95      1.727         24        640: 100% 33/33 [00:20<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.36it/s]\n",
            "                   all        175        227      0.621      0.432      0.479      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/25      2.36G      1.956      2.028      1.727         21        640: 100% 33/33 [00:21<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.05s/it]\n",
            "                   all        175        227      0.443      0.427      0.398      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/25      2.37G      1.883      1.915       1.71         27        640: 100% 33/33 [00:20<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.76it/s]\n",
            "                   all        175        227      0.589      0.533      0.547      0.233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/25      2.37G      1.846      1.817      1.668         20        640: 100% 33/33 [00:20<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.05s/it]\n",
            "                   all        175        227      0.694       0.52      0.559       0.25\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/25      2.54G      1.851      1.889      1.795         17        640: 100% 33/33 [00:23<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.71it/s]\n",
            "                   all        175        227      0.687      0.502       0.56      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/25      2.36G      1.746      1.711      1.715         18        640: 100% 33/33 [00:22<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.06s/it]\n",
            "                   all        175        227      0.733      0.495      0.534      0.249\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/25      2.37G      1.757      1.665      1.746         17        640: 100% 33/33 [00:23<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:06<00:00,  1.04s/it]\n",
            "                   all        175        227      0.749      0.564      0.654       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/25      2.36G      1.746      1.593      1.717         19        640: 100% 33/33 [00:20<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.62it/s]\n",
            "                   all        175        227      0.732      0.541      0.625      0.305\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/25      2.36G      1.712      1.507      1.673         19        640: 100% 33/33 [00:20<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.28it/s]\n",
            "                   all        175        227      0.689      0.555      0.633      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/25      2.36G      1.666       1.48      1.636         16        640: 100% 33/33 [00:21<00:00,  1.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.91it/s]\n",
            "                   all        175        227      0.778      0.599      0.685       0.34\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/25      2.37G      1.626      1.458      1.613         16        640: 100% 33/33 [00:20<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.53it/s]\n",
            "                   all        175        227      0.753      0.606      0.654      0.328\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/25      2.36G      1.616      1.423        1.6         15        640: 100% 33/33 [00:21<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.58it/s]\n",
            "                   all        175        227       0.81      0.581      0.669      0.334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/25      2.36G      1.574      1.394      1.574         16        640: 100% 33/33 [00:24<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:05<00:00,  1.07it/s]\n",
            "                   all        175        227      0.729      0.612      0.681      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/25      2.36G      1.539      1.348      1.545         15        640: 100% 33/33 [00:20<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.63it/s]\n",
            "                   all        175        227      0.835      0.568      0.686      0.361\n",
            "\n",
            "25 epochs completed in 0.204 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme/weights/best.pt...\n",
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:08<00:00,  1.41s/it]\n",
            "                   all        175        227      0.838      0.573      0.688      0.361\n",
            "Speed: 0.4ms preprocess, 7.3ms inference, 0.0ms loss, 14.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**valid**"
      ],
      "metadata": {
        "id": "SXpSrD8oktfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# original\n",
        "model_original = YOLO('/content/drive/MyDrive/CCTV-People-Dataset/runs/train/original_2/weights/best.pt')\n",
        "results_original = model_original.val(data='/content/drive/MyDrive/CCTV-People-Dataset/data_origin.yaml')\n",
        "\n",
        "# enlighten_moderate\n",
        "model_moderate = YOLO('/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_moderate/weights/best.pt')\n",
        "results_moderate = model_moderate.val(data='/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_moderate.yaml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIoEyaYwahcH",
        "outputId": "c77ed709-6be3-4257-bc2c-ad408028672d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 13.5MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/CCTV-People-Dataset/valid/labels.cache... 175 images, 0 backgrounds, 0 corrupt: 100%|██████████| 175/175 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [01:43<00:00,  9.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        175        227      0.855      0.755      0.842      0.492\n",
            "Speed: 17.9ms preprocess, 536.0ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "Ultralytics 8.3.41 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/CCTV-People-Dataset/valid/Low_Brightness_Moderate/enlightengan/labels.cache... 175 images, 0 backgrounds, 0 corrupt: 100%|██████████| 175/175 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [02:06<00:00, 11.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        175        227      0.888      0.749      0.824       0.47\n",
            "Speed: 11.8ms preprocess, 650.9ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# enlighten_extreme\n",
        "model_severe = YOLO('/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_severe/weights/best.pt')\n",
        "results_severe = model_severe.val(data='/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_severe.yaml')\n",
        "\n",
        "# enlighten_extreme\n",
        "model_extreme = YOLO('/content/drive/MyDrive/CCTV-People-Dataset/runs/train/enlightengan_extreme/weights/best.pt')\n",
        "results_extreme = model_extreme.val(data='/content/drive/MyDrive/CCTV-People-Dataset/data_enlightengan_extreme.yaml')"
      ],
      "metadata": {
        "id": "qZKCLGcew4D7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}